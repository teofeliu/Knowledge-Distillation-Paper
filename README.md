# Knowledge Distillation Research Paper

This repository contains the research conducted during the summer of 2023 at Duke University, focusing on the theory behind knowledge distillation in neural networks. This paper marks the initial phase of my research journey, laying the foundation with comprehensive theory-based background research. It served as a precursor to several months of applied research on Knowledge Distillation, where we delved into practical implementations and experimentation using PyTorch. Knowledge distillation is a promising technique in deep learning that enables the transfer of knowledge from a large, complex teacher model to a smaller, more efficient student model while preserving high levels of performance. The aim of this paper is to provide a comprehensive overview of knowledge distillation, examining key concepts, algorithms, computational complexity reduction, and accuracy preservation in the distilled student models.

## Research Details

- **Researcher:** Teo Feliu
- **Date:** May 03, 2023
- **University:** Duke University

## Usage

The **[Paper.pdf](Paper.pdf)** file provides an in-depth exploration of knowledge distillation in neural networks, focusing on the theory and implications. It serves as a valuable resource for researchers and practitioners interested in understanding the foundations of knowledge distillation and its practical applications.

For the PyTorch implementation of applied knowledge distillation, refer to the **Code** directory. The code provides a practical example of how to leverage knowledge distillation techniques in deep learning projects using PyTorch. Please follow the instructions within the code repository for usage guidelines and further details.

## License

This repository is licensed under the [MIT License](LICENSE), granting you the freedom to use, modify, and distribute the research paper and code as per the terms outlined in the license.

## Acknowledgments

We would like to express our gratitude to the faculty, advisors, and researchers at Duke University for their guidance and support during this research project. Additionally, we acknowledge the contributions of the authors whose papers were referenced and analyzed in this research.

---

This repository serves as a starting point for understanding knowledge distillation in deep learning. Feel free to explore the research paper and code implementation. Should you have any questions or suggestions, please don't hesitate to reach out.
